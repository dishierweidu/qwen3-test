# configs/model/qwen3_omni_1_3b_moe.yaml
model_type: qwen3_omni_moe

vocab_size: 152064
hidden_size: 2048
intermediate_size: 5376    # ≈ 8/3 * hidden, 四舍五入到 256 的倍数
num_hidden_layers: 24
num_attention_heads: 16     # head_dim = 128
num_key_value_heads: 16
max_position_embeddings: 4096
rope_theta: 1000000.0

use_moe: true
num_experts: 4              # 全局占位，实际以 thinker_config 为准
num_experts_per_tok: 2

thinker_config:
  hidden_size: 2048
  intermediate_size: 5376
  num_hidden_layers: 24
  num_attention_heads: 16
  num_key_value_heads: 16
  max_position_embeddings: 4096

  use_moe: true
  num_experts: 4
  num_experts_per_tok: 2
  moe_layer_indices: "4,8,12,16,20"  # 每 4 层一层 MoE
  moe_aux_loss_coef: 0.02

  vision_hidden_size: 1152
  audio_hidden_size: 1024
