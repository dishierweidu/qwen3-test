# configs/train/stage1_text_only.yaml

# 阶段1：文本-only（Thinker-Text）

experiment_name: "qwen3_omni_stage1_text_30b"

model:
  config_path: "configs/model/qwen3_omni_30b_moe.yaml"

data:
  train_corpus_paths: 
    - "data/corpus/train_text.jsonl"
    # - "/ai/finnox/mobvoi_seq_monkey_general_open_corpus.jsonl"
    - "/hpc2hdd/home/zzhao056/mobv/mobvoi_seq_monkey_general_open_corpus.jsonl"
  val_corpus_paths:
    - "data/corpus/val_text.jsonl"
  max_seq_length: 2048
  num_workers: 4
  batch_size: 1           # per-GPU batch size
  shuffle: false
  # DDP 性能开关：若你确认每个 iteration 的 forward 都会用到全部参数，可设为 false 提速。
  # 对 MoE/条件分支/多模态路由模型，可能存在“某些 step unused 参数”，建议保持 true。
  ddp_find_unused_parameters: true
  # packed 模式配置
  use_packed_dataset: false
  packed_train_bin_path: "data/corpus/train_packed.bin"
  packed_val_bin_path: "data/corpus/val_packed.bin"
  packed_seq_length: 2048   # 可以和 max_seq_length 一致

train:
  num_epochs: 10
  max_steps: -1           # -1 表示按 epoch 走
  gradient_accumulation_steps: 8
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_ratio: 0.03
  logging_steps: 20
  eval_steps: 1000
  save_steps: 10000
  output_dir: "outputs/stage1_text_30b"
  fp16: false
  bf16: true
  fp8: false
  int8_optimizer: false
  seed: 42
  ddp: true              # 先做单卡/单机，后面再加 FSDP / DeepSpeed
  # 显存不够时可打开，激活检查点（前向重算换显存）。对长序列/大模型建议开启。
  gradient_checkpointing: false
  
  # =========================================================================
  # Accelerator 配置 (可选，与原有 deepspeed/ddp 共存)
  # =========================================================================
  # 启用 Accelerator 统一接口 (支持 DDP/DeepSpeed/FSDP)
  use_accelerator: false
  # Accelerator 配置文件路径 (可选，不指定则使用默认配置)
  # 可选配置文件:
  #   - configs/accelerate/multi_gpu_ddp.yaml      (纯 DDP)
  #   - configs/accelerate/deepspeed_zero3.yaml   (DeepSpeed ZeRO-3)
  #   - configs/accelerate/deepspeed_zero3_offload.yaml (ZeRO-3 + CPU Offload)
  #   - configs/accelerate/fsdp_full_shard.yaml   (FSDP 完全分片)
  accelerator_config_path: null
