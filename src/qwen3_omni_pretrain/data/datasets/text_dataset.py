# src/qwen3_omni_pretrain/data/datasets/text_dataset.py

import numpy as np
import json
from typing import List, Dict

import torch
from torch.utils.data import Dataset
from transformers import PreTrainedTokenizerBase


class TextJsonlDataset(Dataset):
    """
    æ¯è¡Œ: {"text": "..."} çš„ jsonl æ–‡æœ¬è¯­æ–™
    """

    def __init__(self, path: str, tokenizer: PreTrainedTokenizerBase, max_seq_length: int = 2048):
        self.path = path
        self.tokenizer = tokenizer
        self.max_seq_length = max_seq_length

        self.samples: List[str] = []
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                obj = json.loads(line)
                text = obj.get("text", "")
                if text:
                    self.samples.append(text)

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        # ğŸ‘‰ è¿™é‡Œç›´æ¥åš tokenizeï¼Œè®© DataLoader çš„ num_workers å¹¶è¡Œè·‘
        text = self.samples[idx]

        encoded = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_seq_length,
            padding=False,              # ä¸åœ¨è¿™é‡Œ padï¼Œç•™ç»™ collator
            return_attention_mask=True,
        )

        input_ids = torch.tensor(encoded["input_ids"], dtype=torch.long)
        attention_mask = torch.tensor(encoded["attention_mask"], dtype=torch.long)

        return {
            "input_ids": input_ids,         # [T]
            "attention_mask": attention_mask,  # [T]
        }
        
class PackedTokenDataset(Dataset):
    """
    ç¦»çº¿ pre-tokenization åçš„ packed token æ•°æ®é›†ï¼š
      - ä» <prefix>.bin ä¸­ç”¨ np.memmap æŠŠæ‰€æœ‰ token è¯»è¿›æ¥ï¼ˆåªè¯»ï¼‰
      - å›ºå®š seq_length åˆ‡æˆ [N, seq_length] çš„æ ·æœ¬
      - æ¯ä¸ªæ ·æœ¬åªè¿”å› input_idsï¼Œlabels/attention_mask äº¤ç»™ collator å¤„ç†

    bin æ–‡ä»¶æ ¼å¼ï¼š
      - int32 è¿ç»­ token æµ
      - å…ƒä¿¡æ¯ï¼ˆpad/eos/seq_length ç­‰ï¼‰å­˜æ”¾åœ¨ <prefix>.meta.jsonï¼Œä»…ä½œå‚è€ƒ
    """

    def __init__(self, bin_path: str, seq_length: int):
        super().__init__()
        self.bin_path = bin_path
        self.seq_length = int(seq_length)

        assert self.seq_length > 0, "seq_length must be positive."

        # memmapï¼Œä¸ä¼šä¸€æ¬¡æ€§æŠŠæ‰€æœ‰æ•°æ® load è¿›å†…å­˜
        self.tokens = np.memmap(self.bin_path, dtype=np.int32, mode="r")
        self.total_tokens = int(self.tokens.shape[0])

        # ç›´æ¥å‘ä¸‹å–æ•´ï¼Œä¸¢æ‰æœ«å°¾ä¸è¶³ä¸€ä¸ª seq çš„éƒ¨åˆ†
        self.num_sequences = self.total_tokens // self.seq_length

        if self.num_sequences == 0:
            raise ValueError(
                f"PackedTokenDataset: total_tokens={self.total_tokens} < seq_length={self.seq_length}"
            )

        print(
            f"[PackedTokenDataset] {self.bin_path}: "
            f"total_tokens={self.total_tokens}, "
            f"seq_length={self.seq_length}, "
            f"num_sequences={self.num_sequences}"
        )

    def __len__(self) -> int:
        return self.num_sequences

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        if idx < 0 or idx >= self.num_sequences:
            raise IndexError(f"Index {idx} out of range for {self.num_sequences} sequences.")

        start = idx * self.seq_length
        end = start + self.seq_length

        # æ³¨æ„ï¼šmemmap åˆ‡ç‰‡è¿˜æ˜¯è§†å›¾ï¼Œè¿™é‡Œè½¬æˆç‹¬ç«‹ np.arrayï¼Œé¿å…åç»­ä¿®æ”¹å½±å“åº•å±‚
        np_slice = np.array(self.tokens[start:end], dtype=np.int64)
        input_ids = torch.from_numpy(np_slice)  # [seq_length]

        return {"input_ids": input_ids}
